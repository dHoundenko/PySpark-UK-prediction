{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-07T22:20:43.557560Z",
     "start_time": "2023-05-07T22:20:42.484126Z"
    }
   },
   "source": [
    "# Imports\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.sql import functions as f\n",
    "from pyspark.sql.functions import udf, StringType, col, when, max, min, rand, hour, minute, expr\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.ml.classification import MultilayerPerceptronClassifier\n",
    "from pyspark.ml.feature import OneHotEncoder, VectorAssembler, StringIndexer\n",
    "from functools import reduce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-07T22:20:54.648547Z",
     "start_time": "2023-05-07T22:20:43.560180Z"
    }
   },
   "outputs": [],
   "source": [
    "# Building session now\n",
    "spark = SparkSession.builder.appName('deep_learning_with_spark').getOrCreate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-07T22:21:19.940615Z",
     "start_time": "2023-05-07T22:20:54.654394Z"
    }
   },
   "outputs": [],
   "source": [
    "# Load data from csv\n",
    "df1 = spark.read.csv('/mnt/bdpa23-group14-pvc/accidents_2005_to_2007.csv', header=True, inferSchema=True)\n",
    "df2 = spark.read.csv('/mnt/bdpa23-group14-pvc/accidents_2009_to_2011.csv', header=True, inferSchema=True)\n",
    "df3 = spark.read.csv('/mnt/bdpa23-group14-pvc/accidents_2012_to_2014.csv', header=True, inferSchema=True)\n",
    "# Combine the DataFrames into a single DataFrame\n",
    "data = df1.union(df2).union(df3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-07T22:21:24.245896Z",
     "start_time": "2023-05-07T22:21:19.943407Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1504150\n"
     ]
    }
   ],
   "source": [
    "# Drop 2 columns containing lots of null values and then drop remaining rows\n",
    "# that contain null values. Most rows survive\n",
    "print(data.count())\n",
    "data = data.withColumn('Accident_Severity', when(data.Accident_Severity == 1, 0).otherwise(data.Accident_Severity))\n",
    "data = data.withColumn('Accident_Severity', when(data.Accident_Severity == 2, 1).otherwise(data.Accident_Severity))\n",
    "data = data.withColumn('Accident_Severity', when(data.Accident_Severity == 3, 2).otherwise(data.Accident_Severity))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-07T22:21:35.374215Z",
     "start_time": "2023-05-07T22:21:24.250366Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1280205\n",
      "204504\n",
      "19441\n"
     ]
    }
   ],
   "source": [
    "df = data\n",
    "# separate the dataframe into two based on the labels\n",
    "df_majority = df.filter(col(\"Accident_Severity\") == 2)\n",
    "df_minority_1 = df.filter(col(\"Accident_Severity\") == 1)\n",
    "df_minority_2 = df.filter(col(\"Accident_Severity\") == 0)\n",
    "majority = df_majority.count()\n",
    "minority_1 = df_minority_1.count()\n",
    "minority_2 = df_minority_2.count()\n",
    "print(majority)\n",
    "print(minority_1)\n",
    "print(minority_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-07T22:21:35.423490Z",
     "start_time": "2023-05-07T22:21:35.417588Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.015185849141348457 0.09506415522434769\n"
     ]
    }
   ],
   "source": [
    "# Downsample frequent classes\n",
    "ratio_majority = float(minority_2 / majority)\n",
    "ratio_minority_1 = float(minority_2 / minority_1)\n",
    "print(ratio_majority, ratio_minority_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-07T22:21:42.123097Z",
     "start_time": "2023-05-07T22:21:35.426290Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19410\n",
      "19625\n",
      "19441\n"
     ]
    }
   ],
   "source": [
    "# Resample to get balanced data between classes\n",
    "df_undersampled_minority_1 = df_minority_1.sample(False, ratio_minority_1, seed=42)\n",
    "df_undersampled_majority = df_majority.sample(False, ratio_majority, seed=42)\n",
    "print(df_undersampled_majority.count())\n",
    "print(df_undersampled_minority_1.count())\n",
    "print(df_minority_2.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-07T22:21:42.229100Z",
     "start_time": "2023-05-07T22:21:42.125650Z"
    }
   },
   "outputs": [],
   "source": [
    "# Combine into a new dataframe\n",
    "balanced_df = df_undersampled_minority_1.union(df_undersampled_majority).union(df_minority_2)\n",
    "#balanced_df = df_undersampled_majority.union(df_minority_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-07T22:21:42.237894Z",
     "start_time": "2023-05-07T22:21:42.231393Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Accident_Index: string (nullable = true)\n",
      " |-- Location_Easting_OSGR: integer (nullable = true)\n",
      " |-- Location_Northing_OSGR: integer (nullable = true)\n",
      " |-- Longitude: double (nullable = true)\n",
      " |-- Latitude: double (nullable = true)\n",
      " |-- Police_Force: integer (nullable = true)\n",
      " |-- Accident_Severity: integer (nullable = true)\n",
      " |-- Number_of_Vehicles: integer (nullable = true)\n",
      " |-- Number_of_Casualties: integer (nullable = true)\n",
      " |-- Date: string (nullable = true)\n",
      " |-- Day_of_Week: integer (nullable = true)\n",
      " |-- Time: string (nullable = true)\n",
      " |-- Local_Authority_(District): integer (nullable = true)\n",
      " |-- Local_Authority_(Highway): string (nullable = true)\n",
      " |-- 1st_Road_Class: integer (nullable = true)\n",
      " |-- 1st_Road_Number: integer (nullable = true)\n",
      " |-- Road_Type: string (nullable = true)\n",
      " |-- Speed_limit: integer (nullable = true)\n",
      " |-- Junction_Detail: string (nullable = true)\n",
      " |-- Junction_Control: string (nullable = true)\n",
      " |-- 2nd_Road_Class: integer (nullable = true)\n",
      " |-- 2nd_Road_Number: integer (nullable = true)\n",
      " |-- Pedestrian_Crossing-Human_Control: string (nullable = true)\n",
      " |-- Pedestrian_Crossing-Physical_Facilities: string (nullable = true)\n",
      " |-- Light_Conditions: string (nullable = true)\n",
      " |-- Weather_Conditions: string (nullable = true)\n",
      " |-- Road_Surface_Conditions: string (nullable = true)\n",
      " |-- Special_Conditions_at_Site: string (nullable = true)\n",
      " |-- Carriageway_Hazards: string (nullable = true)\n",
      " |-- Urban_or_Rural_Area: integer (nullable = true)\n",
      " |-- Did_Police_Officer_Attend_Scene_of_Accident: string (nullable = true)\n",
      " |-- LSOA_of_Accident_Location: string (nullable = true)\n",
      " |-- Year: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#balanced_df.show(vertical=True)\n",
    "balanced_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-07T22:21:42.738318Z",
     "start_time": "2023-05-07T22:21:42.239530Z"
    }
   },
   "outputs": [],
   "source": [
    "# For simplicity choose only the target and a single column for building the model\n",
    "# (Plans to choose larger set of features once we figure out errors)\n",
    "balanced_df = balanced_df.select(\"Accident_Severity\",\n",
    "                           \"Day_of_week\",\n",
    "                           \"Speed_limit\",\n",
    "                           \"Urban_or_Rural_Area\",\n",
    "                           \"1st_Road_Class\",\n",
    "                           \"2nd_Road_Class\",\n",
    "                           \"Light_Conditions\",\n",
    "                           \"Weather_Conditions\",\n",
    "                           \"Road_Surface_Conditions\", \n",
    "                           \"Police_Force\",\n",
    "                           \"Number_of_Vehicles\",\n",
    "                           \"Number_of_Casualties\",\n",
    "                           \"Local_Authority_(District)\",\n",
    "                           \"Local_Authority_(Highway)\",\n",
    "                           \"Road_Type\",\n",
    "                           \"Pedestrian_Crossing-Human_Control\",\n",
    "                           \"Pedestrian_Crossing-Physical_Facilities\",\n",
    "                           \"Special_Conditions_at_Site\",\n",
    "                           \"Carriageway_Hazards\",\n",
    "                           \"Did_Police_Officer_Attend_Scene_of_Accident\",\n",
    "                           \"Year\"\n",
    "                           )\n",
    "balanced_df = balanced_df.withColumnRenamed(\"Local_Authority_(District)\", \"Local_Authority_District\")\n",
    "balanced_df = balanced_df.withColumnRenamed(\"Local_Authority_(Highway)\", \"Local_Authority_Highway\")\n",
    "balanced_df = balanced_df.withColumnRenamed(\"Pedestrian_Crossing-Human_Control\", \"Pedestrian_Crossing_Human_Control\")\n",
    "balanced_df = balanced_df.withColumnRenamed(\"Pedestrian_Crossing-Physical_Facilities\", \"Pedestrian_Crossing_Physical_Facilities\")\n",
    "balanced_df = balanced_df.dropna(how='any', thresh=None, subset=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-07T22:21:42.746904Z",
     "start_time": "2023-05-07T22:21:42.742633Z"
    }
   },
   "outputs": [],
   "source": [
    "#print(balanced_df.count())\n",
    "#print(balanced_df.distinct().count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-07T22:21:42.855609Z",
     "start_time": "2023-05-07T22:21:42.822637Z"
    }
   },
   "outputs": [],
   "source": [
    "#functions\n",
    "def get_string_mapping(col_name, dataframe):\n",
    "    print(\"Getting mapping\")\n",
    "    unique_strings = [row[0] for row in dataframe.select(col_name).distinct().collect()]\n",
    "    mapping = dict(zip(unique_strings, range(len(unique_strings))))\n",
    "    return mapping\n",
    "\n",
    "def get_embedding(col_name, mapping, dataframe):\n",
    "    print(\"Getting embedding\")\n",
    "    when_expr = reduce(lambda a, b: a.when(dataframe[col_name] == b, mapping[b]),\n",
    "                       mapping, when(dataframe[col_name].isNull(), None))\n",
    "    dataframe = dataframe.withColumn(col_name + \"_embedded\", when_expr)\n",
    "    dataframe = dataframe.drop(col_name)\n",
    "    dataframe = dataframe.withColumnRenamed(col_name + \"_embedded\", col_name)\n",
    "    return dataframe\n",
    "\n",
    "def normalize_column(col_name, dataframe):\n",
    "    print(\"Normalizing\")\n",
    "    max_value = dataframe.select(max(col_name)).collect()[0][0]\n",
    "    dataframe = dataframe.withColumn(f\"{col_name}_normalized\", expr(\"{} / {}\".format(col_name, max_value))).drop(col_name)\n",
    "    dataframe = dataframe.withColumnRenamed(col_name + \"_normalized\", col_name)\n",
    "    return dataframe \n",
    "\n",
    "def embed_time(df):\n",
    "    print(\"Embedding time\")\n",
    "    # Convert to seconds since midnight\n",
    "    df = df.withColumn('Time', hour('Time') * 3600 + minute('Time') * 60)\n",
    "    return df\n",
    "\n",
    "def get_col_names(df):\n",
    "    col_names = []\n",
    "    for col in df.dtypes:\n",
    "        col_names.append((col[0], col[1]))\n",
    "    return col_names\n",
    "\n",
    "def normalize_column_2(col_name, dataframe):\n",
    "    max_value = dataframe.select(max(col_name)).collect()[0][0]\n",
    "    min_value = dataframe.select(min(col_name)).collect()[0][0]\n",
    "    diff = max_value - min_value\n",
    "    dataframe = dataframe.withColumn(f\"{col_name}_normalized\",\n",
    "                                     expr(\"(({} - {}) / {}) * 2 - 1\".format(col_name, min_value, diff))).drop(col_name)\n",
    "    dataframe = dataframe.withColumnRenamed(col_name + \"_normalized\", col_name)\n",
    "    return dataframe "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-05-07T22:20:43.324Z"
    }
   },
   "outputs": [],
   "source": [
    "def handle_dataframe(df):\n",
    "    df_cols = get_col_names(df)\n",
    "    print(\"Starting dataframe handling\")\n",
    "    for col in df_cols:\n",
    "        col_name = col[0]\n",
    "     \n",
    "        if col_name == \"Accident_Severity\":\n",
    "            df = df.withColumnRenamed(\"Accident_Severity\", \"label\")\n",
    "        else:\n",
    "            print(f\"Handling {col_name}\")\n",
    "            if col[1] == 'string':\n",
    "                if col_name == \"Time\":\n",
    "                    df = embed_time(df)\n",
    "                else:\n",
    "                    mapping = get_string_mapping(col_name, df)\n",
    "                    df = get_embedding(col_name, mapping, df)\n",
    "                    \n",
    "            df = normalize_column_2(col_name, df)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-05-07T22:20:43.325Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting dataframe handling\n",
      "Handling Day_of_week\n",
      "Handling Speed_limit\n",
      "Handling Urban_or_Rural_Area\n",
      "Handling 1st_Road_Class\n",
      "Handling 2nd_Road_Class\n",
      "Handling Light_Conditions\n",
      "Getting mapping\n"
     ]
    }
   ],
   "source": [
    "final_df = handle_dataframe(balanced_df)\n",
    "final_df.show(3, vertical=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-05-07T22:20:43.326Z"
    }
   },
   "outputs": [],
   "source": [
    "back_up = final_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-05-07T22:20:43.327Z"
    }
   },
   "outputs": [],
   "source": [
    "col_names = get_col_names(balanced_df)\n",
    "print(col_names)\n",
    "for name in col_names:\n",
    "    col_name = name[0]\n",
    "    unique_strings = [row[0] for row in balanced_df.select(col_name).distinct().collect()]\n",
    "    print(f\"Column {col_name} has {len(unique_strings)} unique strings!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-05-07T22:20:43.328Z"
    }
   },
   "outputs": [],
   "source": [
    "#final_df.write.csv(\"fully_embedded\")\n",
    "#final_df.distinct().count()\n",
    "#final_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-05-07T22:20:43.328Z"
    }
   },
   "outputs": [],
   "source": [
    "# Split dataset\n",
    "train, validation, test = final_df.randomSplit([0.7, 0.2, 0.1], 1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-05-07T22:20:43.329Z"
    }
   },
   "outputs": [],
   "source": [
    "categorical_columns = [item[0] for item in final_df.dtypes if item[1].startswith('string')]\n",
    "numeric_columns = [item[0] for item in final_df.dtypes if item[1].startswith('double')]\n",
    "indexers = [StringIndexer(inputCol=column, outputCol='{0}_index'.format(column)) for column in categorical_columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-05-07T22:20:43.330Z"
    }
   },
   "outputs": [],
   "source": [
    "print(categorical_columns, numeric_columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-05-07T22:20:43.331Z"
    }
   },
   "outputs": [],
   "source": [
    "# Establish features as per Exercise 4\n",
    "featuresCreator = VectorAssembler(inputCols=[indexer.getOutputCol() for indexer in indexers] + numeric_columns, outputCol=\"features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-05-07T22:20:43.332Z"
    }
   },
   "outputs": [],
   "source": [
    "# Define layer structure and calssifier to be used\n",
    "layers = [len(featuresCreator.getInputCols()), 10, 10, 3]\n",
    "classifier = CClassifier(labelCol='label', featuresCol='features', maxIter=100, layers=layers, blockSize=128, stepSize=0.1, seed=1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-05-07T22:20:43.332Z"
    }
   },
   "outputs": [],
   "source": [
    "# Define pipeline\n",
    "pipeline = Pipeline(stages=indexers + [featuresCreator, classifier])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-05-07T22:20:43.333Z"
    }
   },
   "outputs": [],
   "source": [
    "# Train model\n",
    "model = pipeline.fit(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-05-07T22:20:43.334Z"
    }
   },
   "outputs": [],
   "source": [
    "train_output_df = model.transform(train)\n",
    "validation_output_df = model.transform(validation)\n",
    "test_output_df = model.transform(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-05-07T22:20:43.335Z"
    }
   },
   "outputs": [],
   "source": [
    "train_predictionAndLabels = train_output_df.select(\"prediction\", \"label\")\n",
    "validation_predictionAndLabels = validation_output_df.select(\"prediction\",\"label\")\n",
    "test_predictionAndLabels = test_output_df.select(\"prediction\", \"label\")\n",
    "metrics = ['weightedPrecision', 'weightedRecall', 'accuracy']\n",
    "for metric in metrics:\n",
    "    evaluator = MulticlassClassificationEvaluator(metricName=metric)\n",
    "    print('Train ' + metric + ' = ' +\n",
    "    str(evaluator.evaluate(train_predictionAndLabels)))\n",
    "    print('Validation ' + metric + ' = ' +\n",
    "    str(evaluator.evaluate(validation_predictionAndLabels)))\n",
    "    print('Test ' + metric + ' = ' +\n",
    "    str(evaluator.evaluate(test_predictionAndLabels)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-05-07T22:20:43.336Z"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import itertools\n",
    "def plot_confusion_matrix(cm, classes, normalize=False, title='Confusion matrix', cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "        print(cm)\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "        horizontalalignment=\"center\",\n",
    "        color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-05-07T22:20:43.336Z"
    }
   },
   "outputs": [],
   "source": [
    "#Get Class labels\n",
    "class_temp = test_predictionAndLabels.select(\"label\").groupBy(\"label\").count().sort('label', ascending=False).toPandas()\n",
    "print(class_temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-05-07T22:20:43.337Z"
    }
   },
   "outputs": [],
   "source": [
    "#Calculate confusion matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "y_true = test_predictionAndLabels.select(\"label\")\n",
    "y_true = y_true.toPandas()\n",
    "y_pred = test_predictionAndLabels.select(\"prediction\")\n",
    "y_pred = y_pred.toPandas()\n",
    "cnf_matrix = confusion_matrix(y_true, y_pred,labels=class_temp['label'])\n",
    "cnf_matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-05-07T22:20:43.337Z"
    }
   },
   "outputs": [],
   "source": [
    "#Plotting Results\n",
    "plt.figure()\n",
    "plot_confusion_matrix(cnf_matrix, classes=class_temp['label'].values, title='Confusion matrix, without normalization')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
